{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for topic modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ellascarola/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ellascarola/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "#pip install nltk # can install on terminal or by uncommenting this line\n",
    "import nltk; nltk.download('punkt'); nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## lda\n",
    "#!pip install gensim # can install by uncommenting this line\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## visualizing LDA--likely need to install\n",
    "#!pip install pyLDAvis # can install by uncommenting this line\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## random\n",
    "import random\n",
    "import string; punctlist = [char for char in string.punctuation] # list of english punctuation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_upper</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>CLEAN &amp; QUIET APT HOME BY THE PARK</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>SKYLIT MIDTOWN CASTLE</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3647</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3831</td>\n",
       "      <td>Cozy Entire Floor of Brownstone</td>\n",
       "      <td>COZY ENTIRE FLOOR OF BROWNSTONE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5022</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              name  \\\n",
       "0  2539                Clean & quiet apt home by the park   \n",
       "1  2595                             Skylit Midtown Castle   \n",
       "2  3647               THE VILLAGE OF HARLEM....NEW YORK !   \n",
       "3  3831                   Cozy Entire Floor of Brownstone   \n",
       "4  5022  Entire Apt: Spacious Studio/Loft by central park   \n",
       "\n",
       "                                         name_upper neighbourhood_group  price  \n",
       "0                CLEAN & QUIET APT HOME BY THE PARK            Brooklyn    149  \n",
       "1                             SKYLIT MIDTOWN CASTLE           Manhattan    225  \n",
       "2               THE VILLAGE OF HARLEM....NEW YORK !           Manhattan    150  \n",
       "3                   COZY ENTIRE FLOOR OF BROWNSTONE            Brooklyn     89  \n",
       "4  ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK           Manhattan     80  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_upper</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>1615764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>2232600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5775</th>\n",
       "      <td>4209595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>4370230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6269</th>\n",
       "      <td>4581788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>4756856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6605</th>\n",
       "      <td>4774658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>6782407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11963</th>\n",
       "      <td>9325951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12824</th>\n",
       "      <td>9787590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13059</th>\n",
       "      <td>9885866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13401</th>\n",
       "      <td>10052289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15819</th>\n",
       "      <td>12797684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16071</th>\n",
       "      <td>12988898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18047</th>\n",
       "      <td>14135050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28889</th>\n",
       "      <td>22275821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id name name_upper neighbourhood_group  price\n",
       "2854    1615764  NaN        NaN           Manhattan    400\n",
       "3703    2232600  NaN        NaN           Manhattan    200\n",
       "5775    4209595  NaN        NaN           Manhattan    225\n",
       "5975    4370230  NaN        NaN           Manhattan    215\n",
       "6269    4581788  NaN        NaN            Brooklyn    150\n",
       "6567    4756856  NaN        NaN            Brooklyn     70\n",
       "6605    4774658  NaN        NaN           Manhattan     40\n",
       "8841    6782407  NaN        NaN            Brooklyn     45\n",
       "11963   9325951  NaN        NaN           Manhattan    190\n",
       "12824   9787590  NaN        NaN           Manhattan    300\n",
       "13059   9885866  NaN        NaN           Manhattan     67\n",
       "13401  10052289  NaN        NaN            Brooklyn     50\n",
       "15819  12797684  NaN        NaN           Manhattan    100\n",
       "16071  12988898  NaN        NaN               Bronx    130\n",
       "18047  14135050  NaN        NaN            Brooklyn     70\n",
       "28889  22275821  NaN        NaN            Brooklyn    110"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = pd.read_csv(\"../public_data/airbnb_text.zip\")\n",
    "ab.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess documents\n",
    "\n",
    "In this case, each name/name_upper, or listing title, we're treating as a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load stopwords list and augment with our own custom ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\",\n",
       " 'apartment',\n",
       " 'new york',\n",
       " 'nyc',\n",
       " 'bronx',\n",
       " 'brooklyn',\n",
       " 'manhattan',\n",
       " 'queens',\n",
       " 'staten island']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "list_stopwords\n",
    "\n",
    "custom_words_toadd = ['apartment', 'new york', 'nyc',\n",
    "                      'bronx', 'brooklyn',\n",
    "                     'manhattan', 'queens', \n",
    "                      'staten island']\n",
    "\n",
    "list_stopwords_new = list_stopwords + custom_words_toadd\n",
    "\n",
    "list_stopwords_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Remove stopwords from lowercase version of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean & quiet apt home by the park',\n",
       " 'skylit midtown castle',\n",
       " 'the village of harlem....new york !',\n",
       " 'cozy entire floor of brownstone',\n",
       " 'entire apt: spacious studio/loft by central park']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['cozy', 'entire', 'floor', 'brownstone']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert to lowercase and a list\n",
    "corpus_lower = ab.name.str.lower().to_list() #first make everything lowercase and put in a list\n",
    "corpus_lower[0:5]\n",
    "\n",
    "## use wordpunct tokenize and filter out with one\n",
    "example_listing = corpus_lower[3]\n",
    "\n",
    "# use list comprehension to run the tokenizer and remove the stop words including custom words you added\n",
    "nostop_listing = [word for word in wordpunct_tokenize(example_listing) \n",
    "                          if word not in list_stopwords_new]\n",
    "nostop_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can also use .apply instead of making it a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 stem and remove non-alpha\n",
    "\n",
    "Other contexts we may want to leave digits in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cozi', 'entir', 'floor', 'brownston']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## initialize stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "## apply to one by iterating\n",
    "## over the tokens in the list\n",
    "example_listing_preprocess = [porter.stem(token) \n",
    "                            for token in nostop_listing \n",
    "                            if token.isalpha() and \n",
    "                            len(token) > 2]\n",
    "\n",
    "example_listing_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cozy entire floor of brownstone'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['cozi', 'entir', 'floor', 'brownston']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_listing\n",
    "example_listing_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Activity 1\n",
    "\n",
    "The above example performed preprocessing on a single Airbnb listing. We want to generalize this preprocessing across all listings.\n",
    "\n",
    "- Embed step two (remove stopwords) and step three (stem) into one or two functions that take in a raw string (eg the raw text of an Airbnb review) and return a preprocessed string \n",
    "- Apply the function iteratively to preprocess all the texts in `corpus_lower`. Output could either be a list where each list element is a string of a list (e.g., `cozy brownstone apt`), or a list of lists where each element is a tokenized string (e.g., `['cozy', 'brownstone', 'apt'])`\n",
    "\n",
    "Output is flexible: it could be a list of lists containing tokenized/stemmed text or a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [clean, quiet, apt, home, park]\n",
       "1                                 [skylit, midtown, castl]\n",
       "2                              [villag, harlem, new, york]\n",
       "3                          [cozi, entir, floor, brownston]\n",
       "4        [entir, apt, spaciou, studio, loft, central, p...\n",
       "                               ...                        \n",
       "48890         [charm, one, bedroom, newli, renov, rowhous]\n",
       "48891         [afford, room, bushwick, east, williamsburg]\n",
       "48892                [sunni, studio, histor, neighborhood]\n",
       "48893                      [time, squar, cozi, singl, bed]\n",
       "48894               [trendi, duplex, heart, hell, kitchen]\n",
       "Name: name, Length: 48879, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[clean, quiet, apt, home, park]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[skylit, midtown, castl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[villag, harlem, new, york]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[cozi, entir, floor, brownston]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[entir, apt, spaciou, studio, loft, central, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name\n",
       "0                    [clean, quiet, apt, home, park]\n",
       "1                           [skylit, midtown, castl]\n",
       "2                        [villag, harlem, new, york]\n",
       "3                    [cozi, entir, floor, brownston]\n",
       "4  [entir, apt, spaciou, studio, loft, central, p..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SETUP\n",
    "list_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "custom_words_toadd = ['apartment', 'new york', 'nyc',\n",
    "                      'bronx', 'brooklyn',\n",
    "                     'manhattan', 'queens', \n",
    "                      'staten island']\n",
    "\n",
    "list_stopwords_new = list_stopwords + custom_words_toadd\n",
    "porter = PorterStemmer()\n",
    "\n",
    "## your code here to define the function(s)\n",
    "def preprocessing (raw_string):\n",
    "    ## remove stopwords\n",
    "    string_token = [word for word in wordpunct_tokenize(raw_string) if word not in list_stopwords_new]\n",
    "\n",
    "    ## stem\n",
    "    string_stem = [porter.stem(token) for token in string_token if token.isalpha() and len(token) > 2]\n",
    "\n",
    "    return string_stem\n",
    "\n",
    "\n",
    "\n",
    "## your code here to apply the function\n",
    "\n",
    "#check to see if there are nas bc that is messing up the apply\n",
    "#ab[ab[\"name\"].isna()] \n",
    "ab_no_na = ab[ab[\"name\"].notna()]\n",
    "corpus_lower_series = ab_no_na.name.str.lower() #leave as a series to use apply\n",
    "\n",
    "pp_string = corpus_lower_series.apply(preprocessing)\n",
    "pp_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## trying to look at it more cleanly\n",
    "pp_string_df = pp_string.to_frame()\n",
    "\n",
    "pp_string_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a document-term matrix and do some basic diagnostics (more manual approach)\n",
    "\n",
    "Here we'll create a DTM first using the raw documents; in the activity, you'll create one using the preprocessed docs\n",
    "that you created in activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define the dtm function and select data to transform into a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function provided\n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    \"\"\" \n",
    "    Function to create dense document-term matrix (DTM) from a list of strings and provided metadata. \n",
    "    A sparse DTM is a list of term_index/doc_index tuples: if a given term occurs in a given doc at least once, \n",
    "        then this count is listed as a tuple; if not, that term/doc pair is omitted. \n",
    "    In a dense DTM, each row is one text (e.g., an Airbnb listing), each column is a term, and \n",
    "        each cell indicates the frequency of that word in that text. \n",
    "    \n",
    "    Parameters:\n",
    "        list_of_strings (Series): each row contains a preprocessed string (need not be tokenized)\n",
    "        metadata (DataFrame): contains document-level covariates\n",
    "    \n",
    "    Returns:\n",
    "        Dense DTM with metadata on left and then one column per word in lexicon\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize a sklearn tokenizer; this helps us tokenize the preprocessed string input\n",
    "    vectorizer = CountVectorizer(lowercase = True) \n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    print('Sparse matrix form:\\n', dtm_sparse[:3]) # take a look at sparse representation\n",
    "    print()\n",
    "    \n",
    "    # switch the dataframe from the sparse representation to the normal dense representation (so we can treat it as regular dataframe)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out ())\n",
    "    print('Dense matrix form:\\n', dtm_dense_named.head()) # take a look at dense representation\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1) # add back document-level covariates\n",
    "\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price_rawdata</th>\n",
       "      <th>name</th>\n",
       "      <th>name_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23821</th>\n",
       "      <td>19227560</td>\n",
       "      <td>Queens</td>\n",
       "      <td>100</td>\n",
       "      <td>Super Cozy!</td>\n",
       "      <td>super cozy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22905</th>\n",
       "      <td>18560625</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>30</td>\n",
       "      <td>Beautiful Private Bedroom by Prospect Park</td>\n",
       "      <td>beautiful private bedroom by prospect park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20426</th>\n",
       "      <td>16289576</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "      <td>Best Location on the Upper West Side! - Part II</td>\n",
       "      <td>best location on the upper west side! - part ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>893413</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>2500</td>\n",
       "      <td>Architecturally Stunning Former Synagogue!</td>\n",
       "      <td>architecturally stunning former synagogue!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790</th>\n",
       "      <td>14882137</td>\n",
       "      <td>Queens</td>\n",
       "      <td>50</td>\n",
       "      <td>Large, beautiful room near Bushwick</td>\n",
       "      <td>large, beautiful room near bushwick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id neighbourhood_group  price_rawdata  \\\n",
       "23821  19227560              Queens            100   \n",
       "22905  18560625            Brooklyn             30   \n",
       "20426  16289576           Manhattan             80   \n",
       "2018     893413           Manhattan           2500   \n",
       "18790  14882137              Queens             50   \n",
       "\n",
       "                                                  name  \\\n",
       "23821                                      Super Cozy!   \n",
       "22905       Beautiful Private Bedroom by Prospect Park   \n",
       "20426  Best Location on the Upper West Side! - Part II   \n",
       "2018        Architecturally Stunning Former Synagogue!   \n",
       "18790              Large, beautiful room near Bushwick   \n",
       "\n",
       "                                            name_lower  \n",
       "23821                                      super cozy!  \n",
       "22905       beautiful private bedroom by prospect park  \n",
       "20426  best location on the upper west side! - part ii  \n",
       "2018        architecturally stunning former synagogue!  \n",
       "18790              large, beautiful room near bushwick  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## filter out na's\n",
    "## for shorter runtime, random sampling of 1000\n",
    "## get metadata for those\n",
    "## and also renaming price col since it's likely to be corpus word\n",
    "ab_small = ab.loc[~ab.name.isnull(),\n",
    "           ['id', 'neighbourhood_group', 'price', 'name']].copy().rename(columns = {'price':\n",
    "            'price_rawdata'}).sample(n = 1000, random_state = 422)\n",
    "\n",
    "ab_small['name_lower'] = ab_small['name'].str.lower()\n",
    "ab_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Execute the dtm function to create the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix form:\n",
      "   (0, 841)\t1\n",
      "  (0, 281)\t1\n",
      "  (1, 152)\t1\n",
      "  (1, 693)\t1\n",
      "  (1, 157)\t1\n",
      "  (1, 205)\t1\n",
      "  (1, 698)\t1\n",
      "  (1, 653)\t1\n",
      "  (2, 165)\t1\n",
      "  (2, 537)\t1\n",
      "  (2, 637)\t1\n",
      "  (2, 856)\t1\n",
      "  (2, 902)\t1\n",
      "  (2, 939)\t1\n",
      "  (2, 774)\t1\n",
      "  (2, 657)\t1\n",
      "  (2, 471)\t1\n",
      "\n",
      "Dense matrix form:\n",
      "    001  10  10m  10min  10mins  1100  12mins  14  15  15min  ...  yoga  york  \\\n",
      "0    0   0    0      0       0     0       0   0   0      0  ...     0     0   \n",
      "1    0   0    0      0       0     0       0   0   0      0  ...     0     0   \n",
      "2    0   0    0      0       0     0       0   0   0      0  ...     0     0   \n",
      "3    0   0    0      0       0     0       0   0   0      0  ...     0     0   \n",
      "4    0   0    0      0       0     0       0   0   0      0  ...     0     0   \n",
      "\n",
      "   you  your  yu  zen  ღღღsteps  法拉盛中心私人房間獨立衛浴  溫馨大套房  獨一無二的紐約閣樓  \n",
      "0    0     0   0    0         0              0      0          0  \n",
      "1    0     0   0    0         0              0      0          0  \n",
      "2    0     0   0    0         0              0      0          0  \n",
      "3    0     0   0    0         0              0      0          0  \n",
      "4    0     0   0    0         0              0      0          0  \n",
      "\n",
      "[5 rows x 970 columns]\n"
     ]
    }
   ],
   "source": [
    "## example application on raw lowercase texts; \n",
    "dtm_nopre = create_dtm(list_of_strings= ab_small.name_lower,\n",
    "                      metadata = ab_small[['id', 'neighbourhood_group', 'price_rawdata']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price_rawdata</th>\n",
       "      <th>001</th>\n",
       "      <th>10</th>\n",
       "      <th>10m</th>\n",
       "      <th>10min</th>\n",
       "      <th>10mins</th>\n",
       "      <th>1100</th>\n",
       "      <th>...</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yu</th>\n",
       "      <th>zen</th>\n",
       "      <th>ღღღsteps</th>\n",
       "      <th>法拉盛中心私人房間獨立衛浴</th>\n",
       "      <th>溫馨大套房</th>\n",
       "      <th>獨一無二的紐約閣樓</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23821</td>\n",
       "      <td>19227560</td>\n",
       "      <td>Queens</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22905</td>\n",
       "      <td>18560625</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20426</td>\n",
       "      <td>16289576</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>893413</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18790</td>\n",
       "      <td>14882137</td>\n",
       "      <td>Queens</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 974 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        id neighbourhood_group  price_rawdata  001  10  10m  10min  \\\n",
       "0  23821  19227560              Queens            100    0   0    0      0   \n",
       "1  22905  18560625            Brooklyn             30    0   0    0      0   \n",
       "2  20426  16289576           Manhattan             80    0   0    0      0   \n",
       "3   2018    893413           Manhattan           2500    0   0    0      0   \n",
       "4  18790  14882137              Queens             50    0   0    0      0   \n",
       "\n",
       "   10mins  1100  ...  yoga  york  you  your  yu  zen  ღღღsteps  法拉盛中心私人房間獨立衛浴  \\\n",
       "0       0     0  ...     0     0    0     0   0    0         0              0   \n",
       "1       0     0  ...     0     0    0     0   0    0         0              0   \n",
       "2       0     0  ...     0     0    0     0   0    0         0              0   \n",
       "3       0     0  ...     0     0    0     0   0    0         0              0   \n",
       "4       0     0  ...     0     0    0     0   0    0         0              0   \n",
       "\n",
       "   溫馨大套房  獨一無二的紐約閣樓  \n",
       "0      0          0  \n",
       "1      0          0  \n",
       "2      0          0  \n",
       "3      0          0  \n",
       "4      0          0  \n",
       "\n",
       "[5 rows x 974 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000, 974)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inclusive</th>\n",
       "      <th>incredible</th>\n",
       "      <th>incredibly</th>\n",
       "      <th>indoor</th>\n",
       "      <th>inn</th>\n",
       "      <th>inq</th>\n",
       "      <th>insane</th>\n",
       "      <th>int</th>\n",
       "      <th>interior</th>\n",
       "      <th>international</th>\n",
       "      <th>interns</th>\n",
       "      <th>invincible</th>\n",
       "      <th>inviting</th>\n",
       "      <th>inwood</th>\n",
       "      <th>island</th>\n",
       "      <th>it</th>\n",
       "      <th>italy</th>\n",
       "      <th>its</th>\n",
       "      <th>jefferson</th>\n",
       "      <th>jewel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inclusive  incredible  incredibly  indoor  inn  inq  insane  int  interior  \\\n",
       "0          0           0           0       0    0    0       0    0         0   \n",
       "1          0           0           0       0    0    0       0    0         0   \n",
       "2          0           0           0       0    0    0       0    0         0   \n",
       "3          0           0           0       0    0    0       0    0         0   \n",
       "4          0           0           0       0    0    0       0    0         0   \n",
       "\n",
       "   international  interns  invincible  inviting  inwood  island  it  italy  \\\n",
       "0              0        0           0         0       0       0   0      0   \n",
       "1              0        0           0         0       0       0   0      0   \n",
       "2              0        0           0         0       0       0   0      0   \n",
       "3              0        0           0         0       0       0   0      0   \n",
       "4              0        0           0         0       0       0   0      0   \n",
       "\n",
       "   its  jefferson  jewel  \n",
       "0    0          0      0  \n",
       "1    0          0      0  \n",
       "2    0          0      0  \n",
       "3    0          0      0  \n",
       "4    0          0      0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## show first set of rows/cols\n",
    "dtm_nopre.head()\n",
    "\n",
    "## show arbitrary later cols in resulting data\n",
    "dtm_nopre.shape\n",
    "dtm_nopre.iloc[0:5, 480:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Use that matrix/column sums to get basic summary stats of top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "in           367\n",
       "room         244\n",
       "private      163\n",
       "bedroom      152\n",
       "apartment    130\n",
       "            ... \n",
       "gay            1\n",
       "gente          1\n",
       "geodesic       1\n",
       "george         1\n",
       "獨一無二的紐約閣樓      1\n",
       "Length: 970, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## summing each col\n",
    "top_terms = dtm_nopre[dtm_nopre.columns[4:]].sum(axis = 0)\n",
    "\n",
    "## sorting from most frequent to least frequent\n",
    "top_terms.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Activity 2: repeat the above but using the preprocessed text data\n",
    "\n",
    "- Stick with the same random sample of 1000 `ab_small`\n",
    "- Apply the preprocessing steps from activity 1 to create a new column in `ab_small` with the preprocessed text (if you got stuck on that, try just removing stopwords)\n",
    "- Use the `create_dtm` function to create a document-term matrix from the preprocessed data\n",
    "- Use colsums to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price_rawdata</th>\n",
       "      <th>name</th>\n",
       "      <th>name_lower</th>\n",
       "      <th>name_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23821</th>\n",
       "      <td>19227560</td>\n",
       "      <td>Queens</td>\n",
       "      <td>100</td>\n",
       "      <td>Super Cozy!</td>\n",
       "      <td>super cozy!</td>\n",
       "      <td>super cozi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22905</th>\n",
       "      <td>18560625</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>30</td>\n",
       "      <td>Beautiful Private Bedroom by Prospect Park</td>\n",
       "      <td>beautiful private bedroom by prospect park</td>\n",
       "      <td>beauti privat bedroom prospect park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20426</th>\n",
       "      <td>16289576</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "      <td>Best Location on the Upper West Side! - Part II</td>\n",
       "      <td>best location on the upper west side! - part ii</td>\n",
       "      <td>best locat upper west side part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>893413</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>2500</td>\n",
       "      <td>Architecturally Stunning Former Synagogue!</td>\n",
       "      <td>architecturally stunning former synagogue!</td>\n",
       "      <td>architectur stun former synagogu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790</th>\n",
       "      <td>14882137</td>\n",
       "      <td>Queens</td>\n",
       "      <td>50</td>\n",
       "      <td>Large, beautiful room near Bushwick</td>\n",
       "      <td>large, beautiful room near bushwick</td>\n",
       "      <td>larg beauti room near bushwick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id neighbourhood_group  price_rawdata  \\\n",
       "23821  19227560              Queens            100   \n",
       "22905  18560625            Brooklyn             30   \n",
       "20426  16289576           Manhattan             80   \n",
       "2018     893413           Manhattan           2500   \n",
       "18790  14882137              Queens             50   \n",
       "\n",
       "                                                  name  \\\n",
       "23821                                      Super Cozy!   \n",
       "22905       Beautiful Private Bedroom by Prospect Park   \n",
       "20426  Best Location on the Upper West Side! - Part II   \n",
       "2018        Architecturally Stunning Former Synagogue!   \n",
       "18790              Large, beautiful room near Bushwick   \n",
       "\n",
       "                                            name_lower  \\\n",
       "23821                                      super cozy!   \n",
       "22905       beautiful private bedroom by prospect park   \n",
       "20426  best location on the upper west side! - part ii   \n",
       "2018        architecturally stunning former synagogue!   \n",
       "18790              large, beautiful room near bushwick   \n",
       "\n",
       "                                   name_pp  \n",
       "23821                           super cozi  \n",
       "22905  beauti privat bedroom prospect park  \n",
       "20426      best locat upper west side part  \n",
       "2018      architectur stun former synagogu  \n",
       "18790       larg beauti room near bushwick  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here \n",
    "\n",
    "## ab_small (making the data frame)\n",
    "ab_small = ab.loc[~ab.name.isnull(),\n",
    "           ['id', 'neighbourhood_group', 'price', 'name']].copy().rename(columns = {'price':\n",
    "            'price_rawdata'}).sample(n = 1000, random_state = 422)\n",
    "\n",
    "ab_small['name_lower'] = ab_small['name'].str.lower()\n",
    "#ab_small.head()\n",
    "\n",
    "\n",
    "## choosing stopwards and loading how to get stems\n",
    "list_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "custom_words_toadd = ['apartment', 'new york', 'nyc',\n",
    "                      'bronx', 'brooklyn',\n",
    "                     'manhattan', 'queens', \n",
    "                      'staten island']\n",
    "\n",
    "list_stopwords_new = list_stopwords + custom_words_toadd\n",
    "porter = PorterStemmer()\n",
    "\n",
    "## your code here to define the function(s)\n",
    "def preprocessing (raw_string):\n",
    "    ## remove stopwords\n",
    "    string_token = [word for word in wordpunct_tokenize(raw_string) if word not in list_stopwords_new]\n",
    "\n",
    "    ## stem\n",
    "    string_stem = [porter.stem(token) for token in string_token if token.isalpha() and len(token) > 2]\n",
    "\n",
    "    return string_stem\n",
    "\n",
    "\n",
    "\n",
    "## your code here to apply the function\n",
    "\n",
    "\n",
    "## wanted to join output\n",
    "ab_small[\"name_pp\"] = ab_small.name_lower.apply(preprocessing).apply(lambda x: \" \".join(x)) ##to have them not be as lists so you can use the create_dtm on them\n",
    "ab_small.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix form:\n",
      "   (0, 646)\t1\n",
      "  (0, 170)\t1\n",
      "  (1, 60)\t1\n",
      "  (1, 516)\t1\n",
      "  (1, 65)\t1\n",
      "  (1, 519)\t1\n",
      "  (1, 480)\t1\n",
      "  (2, 70)\t1\n",
      "  (2, 388)\t1\n",
      "  (2, 697)\t1\n",
      "  (2, 729)\t1\n",
      "  (2, 584)\t1\n",
      "  (2, 482)\t1\n",
      "\n",
      "Dense matrix form:\n",
      "    abcd  abod  access  acidot  acogedor  across  address  ador  aesthet  \\\n",
      "0     0     0       0       0         0       0        0     0        0   \n",
      "1     0     0       0       0         0       0        0     0        0   \n",
      "2     0     0       0       0         0       0        0     0        0   \n",
      "3     0     0       0       0         0       0        0     0        0   \n",
      "4     0     0       0       0         0       0        0     0        0   \n",
      "\n",
      "   afford  ...  yard  year  yellow  yoga  york  zen  ღღღstep  法拉盛中心私人房間獨立衛浴  \\\n",
      "0       0  ...     0     0       0     0     0    0        0              0   \n",
      "1       0  ...     0     0       0     0     0    0        0              0   \n",
      "2       0  ...     0     0       0     0     0    0        0              0   \n",
      "3       0  ...     0     0       0     0     0    0        0              0   \n",
      "4       0  ...     0     0       0     0     0    0        0              0   \n",
      "\n",
      "   溫馨大套房  獨一無二的紐約閣樓  \n",
      "0      0          0  \n",
      "1      0          0  \n",
      "2      0          0  \n",
      "3      0          0  \n",
      "4      0          0  \n",
      "\n",
      "[5 rows x 753 columns]\n"
     ]
    }
   ],
   "source": [
    "## Use the create_dtm function to create a document-term matrix from the preprocessed data\n",
    "\n",
    "dtm_pp = create_dtm(list_of_strings= ab_small.name_pp,\n",
    "                      metadata = ab_small[['id', 'neighbourhood_group', 'price_rawdata']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price_rawdata</th>\n",
       "      <th>abcd</th>\n",
       "      <th>abod</th>\n",
       "      <th>access</th>\n",
       "      <th>acidot</th>\n",
       "      <th>acogedor</th>\n",
       "      <th>across</th>\n",
       "      <th>...</th>\n",
       "      <th>yard</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>zen</th>\n",
       "      <th>ღღღstep</th>\n",
       "      <th>法拉盛中心私人房間獨立衛浴</th>\n",
       "      <th>溫馨大套房</th>\n",
       "      <th>獨一無二的紐約閣樓</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23821</td>\n",
       "      <td>19227560</td>\n",
       "      <td>Queens</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22905</td>\n",
       "      <td>18560625</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20426</td>\n",
       "      <td>16289576</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>893413</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18790</td>\n",
       "      <td>14882137</td>\n",
       "      <td>Queens</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 757 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        id neighbourhood_group  price_rawdata  abcd  abod  access  \\\n",
       "0  23821  19227560              Queens            100     0     0       0   \n",
       "1  22905  18560625            Brooklyn             30     0     0       0   \n",
       "2  20426  16289576           Manhattan             80     0     0       0   \n",
       "3   2018    893413           Manhattan           2500     0     0       0   \n",
       "4  18790  14882137              Queens             50     0     0       0   \n",
       "\n",
       "   acidot  acogedor  across  ...  yard  year  yellow  yoga  york  zen  \\\n",
       "0       0         0       0  ...     0     0       0     0     0    0   \n",
       "1       0         0       0  ...     0     0       0     0     0    0   \n",
       "2       0         0       0  ...     0     0       0     0     0    0   \n",
       "3       0         0       0  ...     0     0       0     0     0    0   \n",
       "4       0         0       0  ...     0     0       0     0     0    0   \n",
       "\n",
       "   ღღღstep  法拉盛中心私人房間獨立衛浴  溫馨大套房  獨一無二的紐約閣樓  \n",
       "0        0              0      0          0  \n",
       "1        0              0      0          0  \n",
       "2        0              0      0          0  \n",
       "3        0              0      0          0  \n",
       "4        0              0      0          0  \n",
       "\n",
       "[5 rows x 757 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000, 757)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pad</th>\n",
       "      <th>palac</th>\n",
       "      <th>para</th>\n",
       "      <th>paradis</th>\n",
       "      <th>park</th>\n",
       "      <th>parkway</th>\n",
       "      <th>part</th>\n",
       "      <th>patio</th>\n",
       "      <th>peac</th>\n",
       "      <th>peach</th>\n",
       "      <th>pendulum</th>\n",
       "      <th>penth</th>\n",
       "      <th>penthous</th>\n",
       "      <th>peopl</th>\n",
       "      <th>perfect</th>\n",
       "      <th>perfectli</th>\n",
       "      <th>person</th>\n",
       "      <th>perstay</th>\n",
       "      <th>pet</th>\n",
       "      <th>photo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pad  palac  para  paradis  park  parkway  part  patio  peac  peach  \\\n",
       "0    0      0     0        0     0        0     0      0     0      0   \n",
       "1    0      0     0        0     1        0     0      0     0      0   \n",
       "2    0      0     0        0     0        0     1      0     0      0   \n",
       "3    0      0     0        0     0        0     0      0     0      0   \n",
       "4    0      0     0        0     0        0     0      0     0      0   \n",
       "\n",
       "   pendulum  penth  penthous  peopl  perfect  perfectli  person  perstay  pet  \\\n",
       "0         0      0         0      0        0          0       0        0    0   \n",
       "1         0      0         0      0        0          0       0        0    0   \n",
       "2         0      0         0      0        0          0       0        0    0   \n",
       "3         0      0         0      0        0          0       0        0    0   \n",
       "4         0      0         0      0        0          0       0        0    0   \n",
       "\n",
       "   photo  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##use columns to summarise\n",
    "\n",
    "dtm_pp.head()\n",
    "\n",
    "## show arbitrary later cols in resulting data\n",
    "dtm_pp.shape\n",
    "dtm_pp.iloc[0:5, 480:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use gensim to more automatically preprocess/estimate a topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Creating the objects to feed the LDA modeling function\n",
    "\n",
    "Different outputs described below: \n",
    "- Tokenized and preprocessed text \n",
    "- Dictionary \n",
    "- Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '!', 1: 'cozy', 2: 'super', 3: 'beautiful', 4: 'bedroom'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out very rare and very common words reduced the length of dictionary from 1047 to 31.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '!', 1: 'cozy', 2: 'beautiful', 3: 'bedroom', 4: 'park'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of documents represented in dictionary format (with omitted words noted):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([(0, 1), (1, 1)], {'super': 1}),\n",
       " ([(2, 1), (3, 1), (4, 1), (5, 1)], {'by': 1, 'prospect': 1}),\n",
       " ([(0, 1), (6, 1), (7, 1)],\n",
       "  {'best': 1,\n",
       "   'ii': 1,\n",
       "   'location': 1,\n",
       "   'on': 1,\n",
       "   'part': 1,\n",
       "   'side': 1,\n",
       "   'upper': 1,\n",
       "   'west': 1}),\n",
       " ([(0, 1)],\n",
       "  {'architecturally': 1, 'former': 1, 'stunning': 1, 'synagogue': 1}),\n",
       " ([(2, 1), (8, 1), (9, 1), (10, 1), (11, 1)], {'bushwick': 1}),\n",
       " ([(4, 1), (8, 1), (9, 1), (12, 1), (13, 2)],\n",
       "  {'bath': 1, 'bed': 1, 'by': 1, 'central': 1, 'college': 1, 'hunter': 1}),\n",
       " ([(9, 1), (11, 1), (14, 1), (15, 1)], {'bohemian': 1, 'brownstone': 1}),\n",
       " ([(16, 1)],\n",
       "  {'fidi': 1, 'huge': 1, 'loft': 1, 'views': 1, 'w': 1, 'water': 1}),\n",
       " ([], {'hillside': 1, 'hotel': 1}),\n",
       " ([(5, 1), (9, 1), (11, 1), (14, 1), (15, 1)], {'airy': 1})]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Step 1: re-tokenize and store in list\n",
    "## here, i'm doing with the raw random sample of text\n",
    "## in activity, you should do with the preprocessed texts\n",
    "text_raw_tokens = [wordpunct_tokenize(one_text) for one_text in \n",
    "                  ab_small.name_lower]\n",
    "\n",
    "\n",
    "## Step 2: use gensim create dictionary - gets all unique words across documents\n",
    "text_raw_dict = corpora.Dictionary(text_raw_tokens)\n",
    "raw_len = len(text_raw_dict) # get length for comparison below\n",
    "\n",
    "### explore first few keys and values\n",
    "### see that key is just an arbitrary counter; value is the word itself\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]}\n",
    "\n",
    "\n",
    "## Step 3: filter out very rare and very common words\n",
    "## here, i'm using the threshold that a word needs to appear in at least\n",
    "## 5% of docs but not more than 95%\n",
    "## this is an integer count of docs so i round\n",
    "lower_bound = round(ab_small.shape[0]*0.05)\n",
    "upper_bound = round(ab_small.shape[0]*0.95)\n",
    "\n",
    "### apply filtering to dictionary\n",
    "text_raw_dict.filter_extremes(no_below = lower_bound,\n",
    "                             no_above = upper_bound)\n",
    "print(f'Filtering out very rare and very common words reduced the \\\n",
    "length of dictionary from {str(raw_len)} to {str(len(text_raw_dict))}.')\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]} # show first five entries after filtering\n",
    "\n",
    "\n",
    "## Step 4: apply dictionary to TOKENIZED texts\n",
    "## this creates a mapping between each word \n",
    "## in a specific listing and the key in the dictionary.\n",
    "## for words that remain in the filtered dictionary,\n",
    "## output is a list where len(list) == n documents\n",
    "## and each element in the list is a list of tuples\n",
    "## containing the mappings\n",
    "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
    "                   for one_text in text_raw_tokens]\n",
    "\n",
    "### can apply doc2bow(one_text, return_missing = True) to print words\n",
    "### eliminated from the listing bc they're not in filtered dictionary.\n",
    "### but feeding that one with missing values to\n",
    "### the lda function can cause errors\n",
    "corpus_fromdict_showmiss = [text_raw_dict.doc2bow(one_text, return_missing = True)\n",
    "                            for one_text in text_raw_tokens]\n",
    "print('Sample of documents represented in dictionary format (with omitted words noted):')\n",
    "corpus_fromdict_showmiss[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2 Estimating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.ldamodel.LdaModel'>\n"
     ]
    }
   ],
   "source": [
    "## Step 5: we're finally ready to estimate the model!\n",
    "## full documentation here - https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "## here, we're feeding the lda function:\n",
    "## (1) the corpus we created from the dictionary,\n",
    "## (2) a parameter we decide on for the number of topics (k),\n",
    "## (3) the dictionary itself,\n",
    "## (4) parameter for number of passes through training data (more means slower), and\n",
    "## (5) parameter that returns, for each word remaining in dict, the topic probabilities.\n",
    "## see documentation for many other arguments you can vary\n",
    "ldamod = gensim.models.ldamodel.LdaModel(corpus_fromdict, \n",
    "                                         num_topics = 5, \n",
    "                                         id2word=text_raw_dict, \n",
    "                                         passes=6, \n",
    "                                         alpha = 'auto',\n",
    "                                         per_word_topics = True)\n",
    "\n",
    "print(type(ldamod))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  Seeing what topics the estimated model discovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.311*\"!\" + 0.075*\".\" + 0.068*\"brooklyn\" + 0.066*\"/\" + 0.065*\"and\" + 0.059*\",\" + 0.059*\"to\" + 0.052*\"the\" + 0.041*\"studio\" + 0.032*\"cozy\"')\n",
      "(1, '0.159*\"apartment\" + 0.115*\"bedroom\" + 0.095*\"studio\" + 0.080*\"1\" + 0.074*\"east\" + 0.071*\"2\" + 0.054*\"beautiful\" + 0.052*\"-\" + 0.051*\"large\" + 0.042*\"with\"')\n",
      "(2, '0.238*\"in\" + 0.135*\"room\" + 0.113*\"private\" + 0.051*\"williamsburg\" + 0.046*\"brooklyn\" + 0.046*\"spacious\" + 0.043*\"the\" + 0.041*\"bedroom\" + 0.038*\"of\" + 0.031*\"apt\"')\n",
      "(3, '0.190*\"-\" + 0.133*\"park\" + 0.113*\"in\" + 0.077*\"room\" + 0.063*\"near\" + 0.058*\"sunny\" + 0.043*\"cozy\" + 0.043*\"2\" + 0.042*\"bedroom\" + 0.030*\"spacious\"')\n",
      "(4, '0.280*\",\" + 0.081*\"to\" + 0.073*\"room\" + 0.070*\"/\" + 0.066*\"manhattan\" + 0.054*\"&\" + 0.044*\"in\" + 0.040*\"cozy\" + 0.031*\"large\" + 0.029*\"near\"')\n"
     ]
    }
   ],
   "source": [
    "## Post-model 1: explore corpus-wide summary of topics\n",
    "### getting the topics and top words; can retrieve diff top words\n",
    "topics = ldamod.print_topics(num_words = 10)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['super', 'cozy', '!'],\n",
       " ['beautiful', 'private', 'bedroom', 'by', 'prospect', 'park'],\n",
       " ['best',\n",
       "  'location',\n",
       "  'on',\n",
       "  'the',\n",
       "  'upper',\n",
       "  'west',\n",
       "  'side',\n",
       "  '!',\n",
       "  '-',\n",
       "  'part',\n",
       "  'ii'],\n",
       " ['architecturally', 'stunning', 'former', 'synagogue', '!'],\n",
       " ['large', ',', 'beautiful', 'room', 'near', 'bushwick']]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 0.7496185),\n",
       "  (1, 0.056587867),\n",
       "  (2, 0.09588674),\n",
       "  (3, 0.048570838),\n",
       "  (4, 0.04933604)],\n",
       " [(0, 0.026072213),\n",
       "  (1, 0.033575874),\n",
       "  (2, 0.10982796),\n",
       "  (3, 0.80170757),\n",
       "  (4, 0.02881645)],\n",
       " [(0, 0.5305534),\n",
       "  (1, 0.041816887),\n",
       "  (2, 0.07074397),\n",
       "  (3, 0.32056078),\n",
       "  (4, 0.036325015)],\n",
       " [(0, 0.61851865),\n",
       "  (1, 0.087528884),\n",
       "  (2, 0.14265949),\n",
       "  (3, 0.075058855),\n",
       "  (4, 0.076234154)],\n",
       " [(0, 0.02159238),\n",
       "  (1, 0.02789292),\n",
       "  (2, 0.047797896),\n",
       "  (3, 0.023554798),\n",
       "  (4, 0.87916195)]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "## Post-model 2: explore topics associated with each document\n",
    "### for each item in our original dictionary, get list of topic probabilities\n",
    "l=[ldamod.get_document_topics(item) for item in corpus_fromdict]\n",
    "### print result\n",
    "text_raw_tokens[0:5]\n",
    "l[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1853643649413928999486622\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1853643649413928999486622_data = {\"mdsDat\": {\"x\": [-0.041849284229768996, 0.06688915335850765, -0.12423284862327208, -0.13584372619913043, 0.2350367056936637], \"y\": [0.12672659695464175, 0.06655819380921929, -0.2094159215684997, 0.07807461837962985, -0.061943487574991324], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [35.85279869207629, 19.68180426964185, 19.201208926227174, 14.072696088980477, 11.191492023074206]}, \"tinfo\": {\"Term\": [\"!\", \",\", \"in\", \"-\", \"apartment\", \"park\", \"room\", \"studio\", \"to\", \"east\", \"/\", \"the\", \"private\", \"brooklyn\", \".\", \"and\", \"bedroom\", \"2\", \"near\", \"1\", \"sunny\", \"williamsburg\", \"manhattan\", \"large\", \"&\", \"beautiful\", \"spacious\", \"cozy\", \"with\", \"of\", \"williamsburg\", \"private\", \"of\", \"in\", \"the\", \"room\", \"spacious\", \"brooklyn\", \"with\", \"and\", \"apt\", \"cozy\", \"bedroom\", \"1\", \".\", \"beautiful\", \"manhattan\", \"large\", \"apartment\", \"studio\", \"/\", \"east\", \"&\", \",\", \"!\", \"2\", \"near\", \"-\", \"sunny\", \"park\", \",\", \"to\", \"&\", \"manhattan\", \"/\", \"near\", \"large\", \"cozy\", \"sunny\", \".\", \"2\", \"room\", \"spacious\", \"apt\", \"of\", \"and\", \"beautiful\", \"in\", \"bedroom\", \"1\", \"-\", \"with\", \"private\", \"apartment\", \"park\", \"east\", \"brooklyn\", \"!\", \"williamsburg\", \"studio\", \"east\", \"apartment\", \"studio\", \"1\", \"beautiful\", \"2\", \"large\", \"bedroom\", \"with\", \"apt\", \"-\", \"sunny\", \"&\", \"/\", \".\", \"brooklyn\", \"cozy\", \"manhattan\", \"park\", \"spacious\", \"and\", \"to\", \"of\", \"!\", \"williamsburg\", \"the\", \",\", \"near\", \"room\", \"private\", \"in\", \"park\", \"-\", \"near\", \"sunny\", \"2\", \"cozy\", \"spacious\", \"to\", \"room\", \"in\", \"beautiful\", \"bedroom\", \"&\", \"apt\", \"private\", \"apartment\", \"manhattan\", \"1\", \"!\", \"the\", \"brooklyn\", \"with\", \"williamsburg\", \"of\", \"studio\", \"/\", \"large\", \"east\", \"and\", \".\", \",\", \"!\", \"and\", \".\", \"brooklyn\", \"the\", \"to\", \"/\", \"studio\", \"manhattan\", \"cozy\", \"sunny\", \"near\", \",\", \"williamsburg\", \"apt\", \"park\", \"1\", \"with\", \"private\", \"large\", \"of\", \"bedroom\", \"apartment\", \"&\", \"2\", \"east\", \"beautiful\", \"spacious\", \"in\", \"-\", \"room\"], \"Freq\": [128.0, 213.0, 357.0, 132.0, 133.0, 68.0, 239.0, 88.0, 88.0, 53.0, 99.0, 74.0, 159.0, 94.0, 71.0, 56.0, 153.0, 84.0, 54.0, 85.0, 68.0, 66.0, 78.0, 65.0, 61.0, 60.0, 87.0, 96.0, 65.0, 57.0, 59.26822575579277, 131.26956366096402, 44.65302834298568, 276.3659752769064, 50.5084672058595, 156.98102454860432, 53.13389343377181, 53.698692921050345, 30.045749812057487, 24.54672768791007, 36.462784703028575, 32.07301169254867, 47.83621120666117, 20.89461117822149, 16.85423909292683, 13.767949513535758, 15.980168441746377, 11.890824096091379, 20.08222387982458, 13.042987882383132, 12.185267634950998, 6.532507048836038, 5.187443539528668, 11.042274047983362, 5.621726940106486, 3.3783205198142863, 1.7874829066967801, 3.2929294601273202, 1.2901638490566263, 0.9675128084400747, 178.56133387993486, 51.54264647700921, 34.70310372116672, 41.87751028795536, 44.535735626283994, 18.249711391842588, 19.46712423175674, 25.293909744700425, 16.505399144762784, 14.758229954490474, 16.808844271904054, 46.23177194550239, 16.75222431253236, 15.860637175525216, 9.528977197563092, 7.003624059886949, 4.95415412277336, 28.02979501597849, 11.956929471044829, 6.385419431868003, 9.388925828771823, 4.488905198601616, 8.875886633332906, 2.4152628622486843, 1.133023846901136, 0.3171884888821228, 0.5054686225204942, 0.593186968116389, 0.2946664797339782, 0.2670837975150514, 46.067867710093466, 98.61880791142661, 59.10512636894596, 49.53483435615331, 33.81855664812527, 44.198120629905496, 32.02798128061804, 71.49344666622761, 26.29613185103784, 25.856825227002584, 32.36521635561131, 16.14909596786563, 13.259096769098427, 17.73634608858752, 12.199162644521907, 11.500368948193952, 7.792038860955917, 4.59978481704789, 3.095962479362426, 3.3221067620443145, 1.6666323779826868, 1.9619120288858471, 0.9888851754573722, 1.9567293596527098, 0.8999452223680519, 0.8162824229262362, 2.0127579657559016, 0.2121089487359433, 0.884027675625781, 0.519938501027679, 0.9710365265695028, 60.42854649325855, 86.74695828000513, 28.51211538934942, 26.41101279848302, 19.615351012662153, 19.75741937823119, 13.638296502064328, 12.966272685567015, 34.934065342790404, 51.69209084719832, 7.747368369804506, 19.346109504228302, 7.390422153175887, 8.781308571290593, 13.231207959986055, 10.770548413217467, 5.205257153784186, 5.138548732495222, 7.621968831603915, 3.6996187686005393, 4.012727064487107, 2.0462496851973686, 1.6202415613395156, 0.892429961032485, 1.0960361044698537, 0.6930538519839741, 0.4538179096296092, 0.3041582093888981, 0.3053103838166826, 0.3619928499337562, 0.39411104533638347, 112.70125141092156, 23.385645976084277, 27.33524363524947, 24.4740572276816, 18.776711181502925, 21.43039133401129, 23.959010562768377, 14.97027641759224, 10.911339744651633, 11.643215078669312, 8.219351067071324, 6.180971172567248, 21.50727576127996, 4.016729235729706, 4.5984297769150695, 3.25250093201659, 4.037781470734742, 3.086633262583076, 5.614186358814853, 2.0660014369220434, 1.6175605406543423, 2.9093365714548507, 1.7713818915842368, 0.7510409904652204, 0.9373244394435607, 0.27997573149440563, 0.3031229270396007, 0.28295443757545546, 0.9060424130360986, 0.27597851521842787, 0.29067001469172143], \"Total\": [128.0, 213.0, 357.0, 132.0, 133.0, 68.0, 239.0, 88.0, 88.0, 53.0, 99.0, 74.0, 159.0, 94.0, 71.0, 56.0, 153.0, 84.0, 54.0, 85.0, 68.0, 66.0, 78.0, 65.0, 61.0, 60.0, 87.0, 96.0, 65.0, 57.0, 66.09980825496402, 159.5107831141255, 57.68088121769297, 357.9649400796888, 74.00806381815153, 239.32155952721465, 87.12947544798826, 94.19131478393349, 65.96366980947738, 56.90794048568066, 91.55998545376204, 96.55959475510551, 153.54203341961676, 85.99119516947276, 71.50886817712244, 60.5911515812785, 78.57406044518544, 65.9057489550178, 133.6582249583016, 88.48151057090625, 99.10941376457487, 53.501697188694926, 61.291107173434916, 213.5177527002905, 128.49486351040105, 84.93796087372954, 54.94238980919198, 132.07000843973404, 68.57502282723938, 68.87754655997877, 213.5177527002905, 88.53132135660782, 61.291107173434916, 78.57406044518544, 99.10941376457487, 54.94238980919198, 65.9057489550178, 96.55959475510551, 68.57502282723938, 71.50886817712244, 84.93796087372954, 239.32155952721465, 87.12947544798826, 91.55998545376204, 57.68088121769297, 56.90794048568066, 60.5911515812785, 357.9649400796888, 153.54203341961676, 85.99119516947276, 132.07000843973404, 65.96366980947738, 159.5107831141255, 133.6582249583016, 68.87754655997877, 53.501697188694926, 94.19131478393349, 128.49486351040105, 66.09980825496402, 88.48151057090625, 53.501697188694926, 133.6582249583016, 88.48151057090625, 85.99119516947276, 60.5911515812785, 84.93796087372954, 65.9057489550178, 153.54203341961676, 65.96366980947738, 91.55998545376204, 132.07000843973404, 68.57502282723938, 61.291107173434916, 99.10941376457487, 71.50886817712244, 94.19131478393349, 96.55959475510551, 78.57406044518544, 68.87754655997877, 87.12947544798826, 56.90794048568066, 88.53132135660782, 57.68088121769297, 128.49486351040105, 66.09980825496402, 74.00806381815153, 213.5177527002905, 54.94238980919198, 239.32155952721465, 159.5107831141255, 357.9649400796888, 68.87754655997877, 132.07000843973404, 54.94238980919198, 68.57502282723938, 84.93796087372954, 96.55959475510551, 87.12947544798826, 88.53132135660782, 239.32155952721465, 357.9649400796888, 60.5911515812785, 153.54203341961676, 61.291107173434916, 91.55998545376204, 159.5107831141255, 133.6582249583016, 78.57406044518544, 85.99119516947276, 128.49486351040105, 74.00806381815153, 94.19131478393349, 65.96366980947738, 66.09980825496402, 57.68088121769297, 88.48151057090625, 99.10941376457487, 65.9057489550178, 53.501697188694926, 56.90794048568066, 71.50886817712244, 213.5177527002905, 128.49486351040105, 56.90794048568066, 71.50886817712244, 94.19131478393349, 74.00806381815153, 88.53132135660782, 99.10941376457487, 88.48151057090625, 78.57406044518544, 96.55959475510551, 68.57502282723938, 54.94238980919198, 213.5177527002905, 66.09980825496402, 91.55998545376204, 68.87754655997877, 85.99119516947276, 65.96366980947738, 159.5107831141255, 65.9057489550178, 57.68088121769297, 153.54203341961676, 133.6582249583016, 61.291107173434916, 84.93796087372954, 53.501697188694926, 60.5911515812785, 87.12947544798826, 357.9649400796888, 132.07000843973404, 239.32155952721465], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.9752, -2.18, -3.2583, -1.4355, -3.1351, -2.0011, -3.0845, -3.0739, -3.6546, -3.8567, -3.461, -3.5893, -3.1895, -4.0178, -4.2327, -4.4349, -4.2859, -4.5815, -4.0574, -4.489, -4.557, -5.1805, -5.411, -4.6555, -5.3306, -5.8399, -6.4765, -5.8655, -6.8025, -7.0903, -1.2726, -2.5151, -2.9107, -2.7228, -2.6613, -3.5534, -3.4888, -3.227, -3.6539, -3.7657, -3.6356, -2.6239, -3.639, -3.6937, -4.2032, -4.5111, -4.8573, -3.1243, -3.9762, -4.6035, -4.218, -4.9559, -4.2742, -5.5757, -6.3327, -7.6058, -7.1398, -6.9798, -7.6795, -7.7777, -2.6027, -1.8416, -2.3535, -2.5301, -2.9118, -2.6441, -2.9662, -2.1632, -3.1634, -3.1802, -2.9557, -3.651, -3.8481, -3.5572, -3.9315, -3.9904, -4.3797, -4.9068, -5.3027, -5.2322, -5.922, -5.7589, -6.444, -5.7615, -6.5382, -6.6358, -5.7333, -7.9835, -6.5561, -7.0869, -6.4622, -2.0206, -1.6591, -2.7718, -2.8483, -3.1458, -3.1386, -3.5092, -3.5597, -2.5686, -2.1768, -4.0747, -3.1596, -4.1219, -3.9495, -3.5395, -3.7453, -4.4724, -4.4853, -4.0911, -4.8139, -4.7326, -5.4061, -5.6395, -6.2359, -6.0304, -6.4887, -6.9121, -7.3123, -7.3085, -7.1382, -7.0532, -1.1683, -2.7409, -2.5848, -2.6954, -2.9604, -2.8282, -2.7167, -3.1869, -3.5032, -3.4383, -3.7865, -4.0715, -2.8246, -4.5025, -4.3673, -4.7136, -4.4973, -4.7659, -4.1677, -5.1674, -5.4121, -4.8251, -5.3212, -6.1793, -5.9577, -7.1661, -7.0866, -7.1555, -5.9917, -7.1804, -7.1286], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9167, 0.8309, 0.7697, 0.767, 0.6437, 0.6041, 0.5312, 0.4638, 0.2394, 0.1849, 0.105, -0.0764, -0.1404, -0.389, -0.4195, -0.4561, -0.5669, -0.6867, -0.8697, -0.8888, -1.0702, -1.0772, -1.4436, -1.9362, -2.1035, -2.1988, -2.3997, -2.6658, -2.9474, -3.2396, 1.4467, 1.0845, 1.0567, 0.9962, 0.8255, 0.5233, 0.406, 0.2859, 0.2012, 0.0475, 0.0055, -0.0187, -0.0234, -0.1277, -0.1751, -0.4695, -0.8784, -0.9217, -0.9272, -0.9748, -1.0183, -1.062, -1.2633, -2.388, -2.482, -3.5025, -3.6021, -3.7527, -3.7876, -4.1775, 1.5006, 1.3462, 1.2467, 1.0986, 1.0671, 0.997, 0.9286, 0.8858, 0.7305, 0.3858, 0.2439, 0.2041, 0.1192, -0.0704, -0.1183, -0.4528, -0.8669, -1.1878, -1.452, -1.6166, -1.8804, -2.1592, -2.4159, -2.5344, -2.6464, -2.857, -3.014, -3.9067, -3.9509, -4.076, -4.2596, 1.8301, 1.5406, 1.305, 1.0068, 0.4953, 0.3743, 0.1064, 0.0399, 0.0366, 0.0258, -0.0959, -0.1105, -0.1545, -0.3834, -0.5286, -0.5575, -0.7534, -0.8565, -0.8639, -1.035, -1.1949, -1.5122, -1.7477, -2.2078, -2.4302, -3.0019, -3.0174, -3.209, -3.2669, -3.325, -4.3339, 2.0589, 1.3007, 1.2284, 0.8423, 0.8185, 0.7715, 0.7701, 0.4133, 0.2158, 0.0746, 0.0686, 0.0052, -0.1053, -0.6107, -0.8013, -0.8629, -0.8685, -0.872, -1.1568, -1.2726, -1.384, -1.776, -2.1335, -2.2119, -2.3166, -3.0627, -3.1077, -3.5398, -3.7891, -3.9807, -4.5234]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5], \"Freq\": [0.046694473507217886, 0.007782412251202981, 0.015564824502405962, 0.06225929800962385, 0.8794125843859368, 0.08157790306922574, 0.5710453214845802, 0.21210254797998693, 0.11420906429691603, 0.016315580613845148, 0.05151796448251506, 0.8383377856700178, 0.009366902633184557, 0.10303592896503012, 0.02271522532209843, 0.0681456759662953, 0.24229573676904992, 0.6587415343408545, 0.2377327516622441, 0.20976419264315657, 0.16781135411452527, 0.37757554675768185, 0.12107830673385755, 0.45404365025196586, 0.18161746010078633, 0.01008985889448813, 0.2421566134677151, 0.24421104926629847, 0.0697745855046567, 0.5814548792054726, 0.05814548792054726, 0.046516390336437806, 0.03531989665327449, 0.20014608103522213, 0.5180251509146926, 0.23546597768849664, 0.011773298884424832, 0.43930600521891267, 0.12300568146129555, 0.035144480417513016, 0.40416152480139966, 0.14963538537369891, 0.014963538537369892, 0.7406951575998096, 0.0822994619555344, 0.014963538537369892, 0.39318485932023295, 0.17474882636454797, 0.28396684284239043, 0.09829621483005824, 0.05460900823892124, 0.23105683973046867, 0.08252029990373881, 0.561138039345424, 0.1320324798459821, 0.3126179778329512, 0.0781544944582378, 0.4624140922112403, 0.12374461622554317, 0.01953862361455945, 0.5733012658743665, 0.010616690108784564, 0.12740028130541478, 0.04246676043513826, 0.25480056261082956, 0.3314015565326099, 0.2589074660411015, 0.08285038913315247, 0.20712597283288117, 0.12427558369972871, 0.13083697093405705, 0.859785808995232, 0.7710252292823926, 0.07821995079676447, 0.002793569671313017, 0.14526562290827688, 0.002793569671313017, 0.18207819788513865, 0.28829047998480284, 0.4855418610270364, 0.030346366314189775, 0.2036295427440951, 0.5345275497032497, 0.06363423210752972, 0.06363423210752972, 0.13999531063656537, 0.03640176568485188, 0.3276158911636669, 0.5278256024303521, 0.10920529705455563, 0.780154516540166, 0.1733676703422591, 0.01733676703422591, 0.01733676703422591, 0.03467353406845182, 0.014518519458720805, 0.014518519458720805, 0.043555558376162415, 0.8711111675232484, 0.043555558376162415, 0.8212610924634052, 0.056422517802829364, 0.006269168644758819, 0.08149919238186464, 0.037615011868552914, 0.6560211303576543, 0.1922100127162554, 0.0041784785373099, 0.1462467488058465, 0.6082901306072734, 0.19511192868535185, 0.0344315168268268, 0.16068041185852505, 0.14692335060873782, 0.6668059758396562, 0.01130179620067214, 0.1695269430100821, 0.014582568970035835, 0.24790367249060918, 0.23332110352057336, 0.3791467932209317, 0.11666055176028668, 0.6891140960708598, 0.013512041099428626, 0.0540481643977145, 0.25672878088914386, 0.01129543741894418, 0.5873627457850974, 0.02259087483788836, 0.14684068644627435, 0.2372041857978278, 0.8925895786629482, 0.015128636926490649, 0.030257273852981298, 0.060514547705962596, 0.4547958002738308, 0.060639440036510776, 0.39415636023732004, 0.030319720018255388, 0.04547958002738309], \"Term\": [\"!\", \"!\", \"!\", \"!\", \"!\", \"&\", \"&\", \"&\", \"&\", \"&\", \",\", \",\", \",\", \",\", \"-\", \"-\", \"-\", \"-\", \".\", \".\", \".\", \".\", \"/\", \"/\", \"/\", \"/\", \"/\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"2\", \"2\", \"and\", \"and\", \"and\", \"and\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"apt\", \"apt\", \"apt\", \"apt\", \"apt\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"bedroom\", \"bedroom\", \"bedroom\", \"bedroom\", \"bedroom\", \"brooklyn\", \"brooklyn\", \"brooklyn\", \"brooklyn\", \"brooklyn\", \"cozy\", \"cozy\", \"cozy\", \"cozy\", \"cozy\", \"east\", \"east\", \"in\", \"in\", \"in\", \"in\", \"in\", \"large\", \"large\", \"large\", \"large\", \"manhattan\", \"manhattan\", \"manhattan\", \"manhattan\", \"manhattan\", \"near\", \"near\", \"near\", \"near\", \"of\", \"of\", \"of\", \"of\", \"of\", \"park\", \"park\", \"park\", \"park\", \"park\", \"private\", \"private\", \"private\", \"private\", \"private\", \"room\", \"room\", \"room\", \"room\", \"spacious\", \"spacious\", \"spacious\", \"spacious\", \"studio\", \"studio\", \"studio\", \"studio\", \"sunny\", \"sunny\", \"sunny\", \"sunny\", \"sunny\", \"the\", \"the\", \"the\", \"the\", \"to\", \"to\", \"to\", \"to\", \"to\", \"williamsburg\", \"williamsburg\", \"williamsburg\", \"williamsburg\", \"with\", \"with\", \"with\", \"with\", \"with\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 5, 2, 4, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1853643649413928999486622\", ldavis_el1853643649413928999486622_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1853643649413928999486622\", ldavis_el1853643649413928999486622_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1853643649413928999486622\", ldavis_el1853643649413928999486622_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_display = gensimvis.prepare(ldamod, corpus_fromdict, text_raw_dict)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Activity 3\n",
    "\n",
    "- Preprocess the texts if you haven't already\n",
    "- Run the topic model with preprocessed texts\n",
    "- Play around with other parameters like `n_topics` to find a configuration that produces useful topics\n",
    "\n",
    "If you get stuck on the preprocessing part, you can use below function and example code for applying it. Then continue as above (start with tokenizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'cozi', 1: 'super', 2: 'beauti', 3: 'bedroom', 4: 'park'}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out very rare and very common words reduced the length of dictionary from 753 to 14.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'cozi', 1: 'beauti', 2: 'bedroom', 3: 'park', 4: 'privat'}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of documents represented in dictionary format (with omitted words noted):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([(0, 1)], {'super': 1}),\n",
       " ([(1, 1), (2, 1), (3, 1), (4, 1)], {'prospect': 1}),\n",
       " ([], {'best': 1, 'locat': 1, 'part': 1, 'side': 1, 'upper': 1, 'west': 1}),\n",
       " ([], {'architectur': 1, 'former': 1, 'stun': 1, 'synagogu': 1}),\n",
       " ([(1, 1), (5, 1), (6, 1), (7, 1)], {'bushwick': 1}),\n",
       " ([(3, 1), (5, 1)],\n",
       "  {'bath': 1, 'bed': 1, 'central': 1, 'colleg': 1, 'hunter': 1}),\n",
       " ([(5, 1), (7, 1)], {'bohemian': 1, 'brownston': 1}),\n",
       " ([(8, 1)], {'fidi': 1, 'huge': 1, 'loft': 1, 'view': 1, 'water': 1}),\n",
       " ([], {'hillsid': 1, 'hotel': 1}),\n",
       " ([(4, 1), (5, 1), (7, 1)], {'airi': 1})]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.ldamodel.LdaModel'>\n",
      "(0, '0.203*\"apt\" + 0.193*\"park\" + 0.170*\"larg\" + 0.147*\"sunni\" + 0.142*\"near\" + 0.085*\"room\" + 0.049*\"beauti\" + 0.004*\"privat\" + 0.002*\"bedroom\" + 0.001*\"studio\"')\n",
      "(1, '0.463*\"room\" + 0.359*\"privat\" + 0.133*\"williamsburg\" + 0.022*\"cozi\" + 0.007*\"bedroom\" + 0.006*\"spaciou\" + 0.005*\"east\" + 0.001*\"sunni\" + 0.001*\"beauti\" + 0.001*\"larg\"')\n",
      "(2, '0.373*\"studio\" + 0.370*\"spaciou\" + 0.076*\"east\" + 0.062*\"apt\" + 0.054*\"sunni\" + 0.030*\"williamsburg\" + 0.014*\"room\" + 0.006*\"bedroom\" + 0.006*\"beauti\" + 0.003*\"near\"')\n",
      "(3, '0.482*\"bedroom\" + 0.273*\"cozi\" + 0.127*\"beauti\" + 0.100*\"east\" + 0.005*\"near\" + 0.002*\"studio\" + 0.002*\"apt\" + 0.002*\"room\" + 0.002*\"sunni\" + 0.001*\"park\"')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['super', 'cozi'],\n",
       " ['beauti', 'privat', 'bedroom', 'prospect', 'park'],\n",
       " ['best', 'locat', 'upper', 'west', 'side', 'part'],\n",
       " ['architectur', 'stun', 'former', 'synagogu'],\n",
       " ['larg', 'beauti', 'room', 'near', 'bushwick']]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 0.13411795), (1, 0.16383415), (2, 0.10677789), (3, 0.59527004)],\n",
       " [(0, 0.28948155), (1, 0.26162603), (2, 0.04450604), (3, 0.4043864)],\n",
       " [(0, 0.25171965), (1, 0.30336767), (2, 0.20041662), (3, 0.2444961)],\n",
       " [(0, 0.25171965), (1, 0.30336767), (2, 0.20041662), (3, 0.2444961)],\n",
       " [(0, 0.78055847), (1, 0.11620485), (2, 0.044564117), (3, 0.05867254)]]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=18536) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el18536133196886561717790166\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el18536133196886561717790166_data = {\"mdsDat\": {\"x\": [-0.1870164621523441, -0.14905670312624866, 0.36446956721979445, -0.028396401941201674], \"y\": [-0.29163490725132246, 0.1282926089444744, -0.07834560417272105, 0.24168790247956917], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [32.58468032664535, 26.715418045495575, 23.633574072231504, 17.06632755562757]}, \"tinfo\": {\"Term\": [\"bedroom\", \"privat\", \"studio\", \"room\", \"spaciou\", \"cozi\", \"park\", \"larg\", \"apt\", \"near\", \"sunni\", \"williamsburg\", \"beauti\", \"east\", \"privat\", \"williamsburg\", \"room\", \"cozi\", \"east\", \"spaciou\", \"bedroom\", \"beauti\", \"sunni\", \"larg\", \"park\", \"near\", \"apt\", \"studio\", \"park\", \"larg\", \"near\", \"apt\", \"sunni\", \"beauti\", \"room\", \"privat\", \"studio\", \"bedroom\", \"east\", \"williamsburg\", \"cozi\", \"spaciou\", \"bedroom\", \"cozi\", \"beauti\", \"east\", \"near\", \"sunni\", \"studio\", \"apt\", \"park\", \"larg\", \"williamsburg\", \"spaciou\", \"room\", \"privat\", \"studio\", \"spaciou\", \"east\", \"sunni\", \"apt\", \"williamsburg\", \"beauti\", \"room\", \"near\", \"bedroom\", \"park\", \"larg\", \"privat\", \"cozi\"], \"Freq\": [161.0, 162.0, 88.0, 241.0, 89.0, 98.0, 72.0, 63.0, 90.0, 54.0, 67.0, 66.0, 60.0, 52.0, 160.30582426996776, 59.338245044468984, 206.9226961491806, 9.6248175848992, 2.1206286206687137, 2.7059379092715954, 3.0473318707783488, 0.43442486083619464, 0.4630455643809235, 0.41589079375372545, 0.35403720572907355, 0.25523733573301494, 0.41318331394354313, 0.33468837236834426, 70.86796424476762, 62.228965347662225, 52.06837964275957, 74.4347505981449, 53.94178325610014, 17.896905506923936, 31.064008715660055, 1.3401626767253236, 0.4776914151375207, 0.7959861365469607, 0.2530625720946294, 0.25338336850356574, 0.36288049741804, 0.2824349616296963, 156.14415275429027, 88.41968868646403, 41.22627530225839, 32.336222682135876, 1.522735517444621, 0.6110771792060888, 0.7081871882525973, 0.6858662557818761, 0.4432040739274963, 0.33255558756358916, 0.2675656080313119, 0.26831420831895886, 0.6488582579277052, 0.4016186225760699, 87.3419851324391, 86.54953096822616, 17.739205427471365, 12.595376410391173, 14.5244704946389, 6.968677887710951, 1.3567404965182865, 3.2175570885008042, 0.7117135674958824, 1.4997782020783703, 0.3497067844336708, 0.27641599444282755, 0.5815867174771081, 0.26662297154248127], \"Total\": [161.0, 162.0, 88.0, 241.0, 89.0, 98.0, 72.0, 63.0, 90.0, 54.0, 67.0, 66.0, 60.0, 52.0, 162.62919228674627, 66.82787190871481, 241.8531202112692, 98.67400974032374, 52.44911930237059, 89.80621804744642, 161.48724896369396, 60.9143461665368, 67.61128241007832, 63.25382772342237, 72.01491230885784, 54.55806606343309, 90.05827066250923, 88.86255210819756, 72.01491230885784, 63.25382772342237, 54.55806606343309, 90.05827066250923, 67.61128241007832, 60.9143461665368, 241.8531202112692, 162.62919228674627, 88.86255210819756, 161.48724896369396, 52.44911930237059, 66.82787190871481, 98.67400974032374, 89.80621804744642, 161.48724896369396, 98.67400974032374, 60.9143461665368, 52.44911930237059, 54.55806606343309, 67.61128241007832, 88.86255210819756, 90.05827066250923, 72.01491230885784, 63.25382772342237, 66.82787190871481, 89.80621804744642, 241.8531202112692, 162.62919228674627, 88.86255210819756, 89.80621804744642, 52.44911930237059, 67.61128241007832, 90.05827066250923, 66.82787190871481, 60.9143461665368, 241.8531202112692, 54.55806606343309, 161.48724896369396, 72.01491230885784, 63.25382772342237, 162.62919228674627, 98.67400974032374], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.0249, -2.0187, -0.7696, -3.8376, -5.3503, -5.1065, -4.9877, -6.9357, -6.8719, -6.9793, -7.1403, -7.4675, -6.9858, -7.1965, -1.6425, -1.7725, -1.9508, -1.5934, -1.9155, -3.0187, -2.4673, -5.6106, -6.6422, -6.1315, -7.2775, -7.2762, -6.917, -7.1677, -0.73, -1.2987, -2.0617, -2.3046, -5.3603, -6.2733, -6.1258, -6.1579, -6.5945, -6.8817, -7.0992, -7.0964, -6.2133, -6.693, -0.9854, -0.9945, -2.5795, -2.9219, -2.7794, -3.5138, -5.1501, -4.2866, -5.7953, -5.0499, -6.5059, -6.7411, -5.9972, -6.7772], \"loglift\": [14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1069, 1.0025, 0.9653, -1.2061, -2.0868, -2.3809, -2.8488, -3.8219, -3.8624, -3.9032, -4.1939, -4.2435, -4.263, -4.4603, 1.3039, 1.3036, 1.2732, 1.1294, 1.0941, 0.0951, -0.7324, -3.4788, -3.906, -3.9927, -4.014, -4.255, -4.2856, -4.442, 1.4089, 1.3328, 1.0521, 0.9588, -2.1363, -3.2638, -3.3896, -3.435, -3.6481, -3.8056, -4.078, -4.3707, -4.4784, -4.5612, 1.7508, 1.7311, 0.684, 0.0876, -0.0566, -0.4926, -2.0363, -2.5516, -2.5713, -2.911, -3.5595, -3.6649, -3.8654, -4.1457]}, \"token.table\": {\"Topic\": [2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 3, 1, 3, 4, 2, 2, 3, 4, 2, 1, 2, 4, 1, 2, 3, 4, 1, 4, 3, 4, 2, 3, 4, 1, 4], \"Freq\": [0.8216902174072703, 0.011103921856855005, 0.16655882785282505, 0.2954968924855385, 0.6730762551059488, 0.01641649402697436, 0.01857731814277466, 0.006192439380924886, 0.9660205434242822, 0.006192439380924886, 0.10134380903661036, 0.8918255195221712, 0.03813219414552885, 0.6101151063284616, 0.34318974730975965, 0.9801778363689746, 0.9531129629767503, 0.036658190883721166, 0.018329095441860583, 0.9859069146053377, 0.9838332082341619, 0.006148957551463512, 0.006148957551463512, 0.8558913766304794, 0.12817696944707663, 0.004134740949905697, 0.012404222849717093, 0.0334052592930151, 0.9687525194974379, 0.01125333423670318, 0.9790400785931767, 0.7986832681634007, 0.014790430891914829, 0.19227560159489276, 0.8828651626164681, 0.1047467142087335], \"Term\": [\"apt\", \"apt\", \"apt\", \"beauti\", \"beauti\", \"beauti\", \"bedroom\", \"bedroom\", \"bedroom\", \"bedroom\", \"cozi\", \"cozi\", \"east\", \"east\", \"east\", \"larg\", \"near\", \"near\", \"near\", \"park\", \"privat\", \"privat\", \"privat\", \"room\", \"room\", \"room\", \"room\", \"spaciou\", \"spaciou\", \"studio\", \"studio\", \"sunni\", \"sunni\", \"sunni\", \"williamsburg\", \"williamsburg\"]}, \"R\": 14, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el18536133196886561717790166\", ldavis_el18536133196886561717790166_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el18536133196886561717790166\", ldavis_el18536133196886561717790166_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el18536133196886561717790166\", ldavis_el18536133196886561717790166_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "## Step 1: re-tokenize and store in list\n",
    "## here, i'm doing with the raw random sample of text\n",
    "## in activity, you should do with the preprocessed texts\n",
    "text_raw_tokens = [wordpunct_tokenize(one_text) for one_text in \n",
    "                  ab_small.name_pp]\n",
    "\n",
    "\n",
    "## Step 2: use gensim create dictionary - gets all unique words across documents\n",
    "text_raw_dict = corpora.Dictionary(text_raw_tokens)\n",
    "raw_len = len(text_raw_dict) # get length for comparison below\n",
    "\n",
    "### explore first few keys and values\n",
    "### see that key is just an arbitrary counter; value is the word itself\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]}\n",
    "\n",
    "\n",
    "## Step 3: filter out very rare and very common words\n",
    "## here, i'm using the threshold that a word needs to appear in at least\n",
    "## 5% of docs but not more than 95%\n",
    "## this is an integer count of docs so i round\n",
    "lower_bound = round(ab_small.shape[0]*0.05)\n",
    "upper_bound = round(ab_small.shape[0]*0.95)\n",
    "\n",
    "### apply filtering to dictionary\n",
    "text_raw_dict.filter_extremes(no_below = lower_bound,\n",
    "                             no_above = upper_bound)\n",
    "print(f'Filtering out very rare and very common words reduced the \\\n",
    "length of dictionary from {str(raw_len)} to {str(len(text_raw_dict))}.')\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]} # show first five entries after filtering\n",
    "\n",
    "\n",
    "## Step 4: apply dictionary to TOKENIZED texts\n",
    "## this creates a mapping between each word \n",
    "## in a specific listing and the key in the dictionary.\n",
    "## for words that remain in the filtered dictionary,\n",
    "## output is a list where len(list) == n documents\n",
    "## and each element in the list is a list of tuples\n",
    "## containing the mappings\n",
    "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
    "                   for one_text in text_raw_tokens]\n",
    "\n",
    "### can apply doc2bow(one_text, return_missing = True) to print words\n",
    "### but feeding that one with missing values to\n",
    "### the lda function can cause errors\n",
    "corpus_fromdict_showmiss = [text_raw_dict.doc2bow(one_text, return_missing = True)\n",
    "                            for one_text in text_raw_tokens]\n",
    "print('Sample of documents represented in dictionary format (with omitted words noted):')\n",
    "corpus_fromdict_showmiss[:10]\n",
    "\n",
    "\n",
    "## Step 5: we're finally ready to estimate the model!\n",
    "## full documentation here - https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "## here, we're feeding the lda function:\n",
    "## (1) the corpus we created from the dictionary,\n",
    "## (2) a parameter we decide on for the number of topics (k),\n",
    "## (3) the dictionary itself,\n",
    "## (4) parameter for number of passes through training data (more means slower), and\n",
    "## (5) parameter that returns, for each word remaining in dict, the topic probabilities.\n",
    "## see documentation for many other arguments you can vary\n",
    "\n",
    "\n",
    "## IN HERE PLAY AROUIND WIHT NUM TIOPICS AND PASSES\n",
    "ldamod = gensim.models.ldamodel.LdaModel(corpus_fromdict, \n",
    "                                         num_topics = 4, \n",
    "                                         id2word=text_raw_dict, \n",
    "                                         passes=10, #increasing the hot and cold idea-->like how nmany questions you would ask (how algorithms convverge)\n",
    "                                         alpha = 'auto',\n",
    "                                         per_word_topics = True)\n",
    "\n",
    "print(type(ldamod))\n",
    "\n",
    "## Post-model 1: explore corpus-wide summary of topics\n",
    "### getting the topics and top words; can retrieve diff top words\n",
    "topics = ldamod.print_topics(num_words = 10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "## Post-model 2: explore topics associated with each document\n",
    "### for each item in our original dictionary, get list of topic probabilities\n",
    "l=[ldamod.get_document_topics(item) for item in corpus_fromdict]\n",
    "### print result\n",
    "text_raw_tokens[0:5]\n",
    "l[0:5]\n",
    "\n",
    "## Display\n",
    "lda_display = gensimvis.prepare(ldamod, corpus_fromdict, text_raw_dict)\n",
    "pyLDAvis.display(lda_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
